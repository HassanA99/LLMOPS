{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fula Dialect Prediction Model\n",
    "# =============================\n",
    "\n",
    "# This notebook demonstrates the process of building a machine learning model\n",
    "# to predict Fula dialects based on text and audio inputs.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multimodal_model import MultiModalClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Exploration\n",
    "# ===============================\n",
    "\n",
    "# Load the hypothetical dataset\n",
    "def load_data():\n",
    "    # In a real scenario, you would load your actual dataset here\n",
    "    # For this example, we'll create a dummy dataset\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"Mi yiɗi nyaameede nyiiri e kosam\",\n",
    "            \"Mi faalaa ɲaameede ɲiiri e kosam\",\n",
    "            \"Miin yiɗi ɲaameede ɲiiri e kosam\",\n",
    "            \"Mi yiɗi nyaameede nyiiri e biraaɗam\",\n",
    "            \"Mi faalaa ɲaameede ɲiiri e biraaɗam\",\n",
    "        ],\n",
    "        'audio_path': [\n",
    "            'path/to/audio1.wav',\n",
    "            'path/to/audio2.wav',\n",
    "            'path/to/audio3.wav',\n",
    "            'path/to/audio4.wav',\n",
    "            'path/to/audio5.wav',\n",
    "        ],\n",
    "        'dialect': ['Pulaar', 'Fulfulde', 'Pular', 'Pulaar', 'Fulfulde']\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_data()\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Visualize dialect distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='dialect', data=df)\n",
    "plt.title('Distribution of Fula Dialects')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Feature Extraction\n",
    "# ==========================\n",
    "\n",
    "# Tokenize and vectorize text data\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "text_features = tfidf.fit_transform(df['text'])\n",
    "\n",
    "print(\"Text features shape:\", text_features.shape)\n",
    "\n",
    "# 3. Audio Feature Extraction\n",
    "# ===========================\n",
    "\n",
    "def extract_audio_features(audio_path):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Compute the mean of each MFCC coefficient\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    \n",
    "    return mfccs_mean\n",
    "\n",
    "# Extract audio features for each file\n",
    "audio_features = np.array([extract_audio_features(path) for path in df['audio_path']])\n",
    "\n",
    "print(\"Audio features shape:\", audio_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Preprocessing\n",
    "# =====================\n",
    "\n",
    "# Encode dialect labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['dialect'])\n",
    "\n",
    "# Combine text and audio features\n",
    "X = np.hstack((text_features.toarray(), audio_features))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# 5. Model Building\n",
    "# =================\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MultiModalClassifier(\n",
    "    text_input_size=1000,\n",
    "    audio_input_size=13,\n",
    "    hidden_size=64,\n",
    "    num_classes=len(le.classes_)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        loss = model.train_step(batch_X, batch_y)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Evaluation\n",
    "# ===================\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Feature Importance Analysis\n",
    "# ==============================\n",
    "\n",
    "# Analyze feature importance for text\n",
    "feature_importance = model.get_text_feature_importance()\n",
    "top_features = pd.DataFrame({\n",
    "    'feature': tfidf.get_feature_names_out(),\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=top_features)\n",
    "plt.title('Top 20 Important Text Features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Error Analysis\n",
    "# =================\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "print(\"Sample of misclassified instances:\")\n",
    "for idx in misclassified_indices[:5]:\n",
    "    print(f\"True: {le.inverse_transform([y_test[idx]])[0]}, Predicted: {le.inverse_transform([y_pred[idx]])[0]}\")\n",
    "    print(f\"Text: {df['text'].iloc[idx]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 9. Model Interpretation\n",
    "# =======================\n",
    "\n",
    "# Use SHAP values for model interpretation\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=tfidf.get_feature_names_out() + ['MFCC_' + str(i) for i in range(13)])\n",
    "\n",
    "# 10. Deployment Preparation\n",
    "# ==========================\n",
    "\n",
    "# Save the model and necessary components\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'fula_dialect_model.joblib')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(le, 'label_encoder.joblib')\n",
    "\n",
    "print(\"Model and components saved successfully.\")\n",
    "#------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Conclusion and Future Work\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "Conclusion:\n",
    "- We've built a multimodal model to predict Fula dialects using both text and audio features.\n",
    "- The model shows promising results in distinguishing between different Fula dialects.\n",
    "- Feature importance analysis reveals key linguistic markers for each dialect.\n",
    "\n",
    "Future Work:\n",
    "1. Collect more diverse and extensive Fula dialect data to improve model performance.\n",
    "2. Experiment with more advanced audio feature extraction techniques, such as using pre-trained audio models.\n",
    "3. Implement data augmentation techniques to handle imbalanced dialect distributions.\n",
    "4. Explore transfer learning approaches by fine-tuning pre-trained language models for Fula.\n",
    "5. Develop a user-friendly interface for real-time dialect prediction.\n",
    "6. Collaborate with Fula language experts to validate and refine the model's predictions.\n",
    "7. Extend the model to handle code-switching and mixed dialect scenarios.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
